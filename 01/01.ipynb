{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic Agent with LangGraph and Groq\n",
    "\n",
    "In this notebook, we will build a simple agent using **LangGraph** and **Groq**'s Llama 3 model. The agent will be capable of performing basic arithmetic operations by deciding when to call specific tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "First, we import the necessary libraries and load our environment variables (like the Groq API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Literal, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.messages import SystemMessage, ToolMessage, HumanMessage, AnyMessage\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Tools\n",
    "\n",
    "We define the arithmetic tools (`multiply`, `add`, `divide`) that our agent can use. These are decorated with `@tool` to make them compatible with LangChain's tool binding system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide `a` and `b`.\n",
    "\n",
    "    Args:\n",
    "        a: First int\n",
    "        b: Second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Create a list of tools for the agent to bind to\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "# Create a mapping for the tool node to find the tool by name\n",
    "tools_by_name = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Model (Groq)\n",
    "\n",
    "We need to initialize the chat model. Here we are using `llama-3.3-70b-versatile` hosted on Groq. We also bind the tools we defined earlier to the model, giving it the ability to call them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use init_chat_model with the Groq provider.\n",
    "model = init_chat_model(\n",
    "    \"llama-3.3-70b-versatile\",\n",
    "    model_provider=\"groq\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Augment the LLM with tools\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define State\n",
    "\n",
    "We define the `MessagesState` which keeps track of the conversation history (list of messages) and any other state variables, like the number of LLM calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Nodes\n",
    "\n",
    "Our graph consists of two main nodes:\n",
    "1.  **`llm_call`**: This node invokes the LLM with the current state. The LLM decides whether to respond directly or request a tool call.\n",
    "2.  **`tool_node`**: If the LLM requests a tool call, this node executes the tool and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: MessagesState):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    }\n",
    "\n",
    "\n",
    "def tool_node(state: MessagesState):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    result = []\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    for tool_call in last_message.tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        # Invoke the tool\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        # Create a ToolMessage with the result\n",
    "        result.append(ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"]))\n",
    "        \n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Logic (Conditional Edges)\n",
    "\n",
    "We need a function to decide the flow of control. After the LLM runs, we check:\n",
    "- If it generated a tool call -> Go to `tool_node`.\n",
    "- If it just generated text -> End the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Agent Graph\n",
    "\n",
    "Now we assemble the StateGraph. We add our nodes and define the edges to create the cycle: `llm_call` -> (conditional) -> `tool_node` -> `llm_call`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Execution\n",
    "\n",
    "Finally, we can test our agent with a query that requires multiple steps (addition followed by multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Running Agent (Groq) ---\\n\")\n",
    "\n",
    "# Example Query\n",
    "user_query = \"Add 3 and 4, then multiply the result by 10.\"\n",
    "print(f\"User: {user_query}\")\n",
    "\n",
    "user_input = [HumanMessage(content=user_query)]\n",
    "\n",
    "# Run the graph\n",
    "output = agent.invoke({\"messages\": user_input})\n",
    "\n",
    "print(\"\\n--- Conversation History ---\\n\")\n",
    "for m in output[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
